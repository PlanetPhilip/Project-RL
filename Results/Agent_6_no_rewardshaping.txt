Transition Profile during evaluationwithout reward shaping:
Q-table shape: (10, 10, 6, 3, 3)
State choice: ['storage_level', 'price', 'hour', 'Day_of_Week']
State bin size: [10, 10, 6, 7]
Bins length: 4
Bins: [array([  0.,  29.,  58.,  87., 116., 145., 174., 203., 232., 261., 290.]), array([1.000000e-02, 2.500090e+02, 5.000080e+02, 7.500070e+02,
       1.000006e+03, 1.250005e+03, 1.500004e+03, 1.750003e+03,
       2.000002e+03, 2.250001e+03, 2.500000e+03]), array([ 1.,  5.,  9., 13., 17., 21., 25.]), array([0.        , 0.85714286, 1.71428571, 2.57142857, 3.42857143,
       4.28571429, 5.14285714, 6.        ])]
Discount rate: 0.95
Small reward: 50000
Large reward: 100000
Learning rate: 0.05
N simulations: 10
State: [ 0.   24.31  1.    1.  ], Action: 1, Reward: -243.1, Next state: [10.   24.31  2.    1.  ]
State: [10.   24.31  2.    1.  ], Action: 1, Reward: -243.1, Next state: [20.   21.71  3.    1.  ]
State: [20.   21.71  3.    1.  ], Action: 1, Reward: -217.10000000000002, Next state: [30.    8.42  4.    1.  ]
State: [30.    8.42  4.    1.  ], Action: 1, Reward: -84.2, Next state: [4.e+01 1.e-02 5.e+00 1.e+00]
State: [4.e+01 1.e-02 5.e+00 1.e+00], Action: 1, Reward: -0.1, Next state: [5.e+01 1.e-02 6.e+00 1.e+00]
State: [5.e+01 1.e-02 6.e+00 1.e+00], Action: 2, Reward: -0.1, Next state: [6.e+01 2.e-02 7.e+00 1.e+00]
State: [6.e+01 2.e-02 7.e+00 1.e+00], Action: 2, Reward: -0.2, Next state: [7.e+01 1.e-02 8.e+00 1.e+00]
State: [7.e+01 1.e-02 8.e+00 1.e+00], Action: 2, Reward: -0.1, Next state: [8.e+01 1.e-02 9.e+00 1.e+00]
State: [8.e+01 1.e-02 9.e+00 1.e+00], Action: 2, Reward: -0.1, Next state: [90.    6.31 10.    1.  ]
State: [90.    6.31 10.    1.  ], Action: 1, Reward: -63.099999999999994, Next state: [100.     7.81  11.     1.  ]
State: [100.     7.81  11.     1.  ], Action: 1, Reward: -78.1, Next state: [110.     9.31  12.     1.  ]
State: [110.     9.31  12.     1.  ], Action: 1, Reward: -93.10000000000001, Next state: [120.   21.7  13.    1. ]
State: [120.   21.7  13.    1. ], Action: 0, Reward: -0.0, Next state: [120.    14.01  14.     1.  ]
State: [120.    14.01  14.     1.  ], Action: 0, Reward: -0.0, Next state: [120.  15.  15.   1.]
State: [120.  15.  15.   1.], Action: 0, Reward: -0.0, Next state: [120.  10.  16.   1.]
State: [120.  10.  16.   1.], Action: 0, Reward: -0.0, Next state: [120.     8.17  17.     1.  ]
State: [120.     8.17  17.     1.  ], Action: 0, Reward: -0.0, Next state: [120.    27.77  18.     1.  ]
State: [120.    27.77  18.     1.  ], Action: 0, Reward: -0.0, Next state: [120.    37.99  19.     1.  ]
State: [120.    37.99  19.     1.  ], Action: 0, Reward: -0.0, Next state: [120.    33.11  20.     1.  ]
State: [120.    33.11  20.     1.  ], Action: 0, Reward: -0.0, Next state: [120.    37.99  21.     1.  ]
State: [120.    37.99  21.     1.  ], Action: 0, Reward: -0.0, Next state: [120.  33.  22.   1.]
State: [120.  33.  22.   1.], Action: 1, Reward: -330.0, Next state: [130.    36.48  23.     1.  ]
State: [130.    36.48  23.     1.  ], Action: 1, Reward: -364.79999999999995, Next state: [140.    30.65  24.     1.  ]
State: [140.    30.65  24.     1.  ], Action: 1, Reward: -306.5, Next state: [30.   16.01  1.    2.  ]
State: [30.   16.01  1.    2.  ], Action: 1, Reward: -160.10000000000002, Next state: [40. 11.  2.  2.]
State: [40. 11.  2.  2.], Action: 1, Reward: -110.0, Next state: [50.    9.01  3.    2.  ]
State: [50.    9.01  3.    2.  ], Action: 1, Reward: -90.1, Next state: [60.   7.5  4.   2. ]
State: [60.   7.5  4.   2. ], Action: 0, Reward: -0.0, Next state: [60.  9.  5.  2.]
State: [60.  9.  5.  2.], Action: 0, Reward: -0.0, Next state: [60.    7.45  6.    2.  ]
State: [60.    7.45  6.    2.  ], Action: 1, Reward: -74.5, Next state: [70.  16.5  7.   2. ]
State: [70.  16.5  7.   2. ], Action: 1, Reward: -165.0, Next state: [80.   28.01  8.    2.  ]
State: [80.   28.01  8.    2.  ], Action: 1, Reward: -280.1, Next state: [90.   29.96  9.    2.  ]
State: [90.   29.96  9.    2.  ], Action: 0, Reward: -0.0, Next state: [90.  39.6 10.   2. ]
State: [90.  39.6 10.   2. ], Action: 0, Reward: -0.0, Next state: [90.   48.15 11.    2.  ]
State: [90.   48.15 11.    2.  ], Action: 0, Reward: -0.0, Next state: [90.   49.93 12.    2.  ]
State: [90.   49.93 12.    2.  ], Action: 0, Reward: -0.0, Next state: [90. 49. 13.  2.]
State: [90. 49. 13.  2.], Action: 0, Reward: -0.0, Next state: [90.   46.01 14.    2.  ]
State: [90.   46.01 14.    2.  ], Action: 2, Reward: -460.09999999999997, Next state: [100.    37.57  15.     2.  ]
State: [100.    37.57  15.     2.  ], Action: 2, Reward: -375.7, Next state: [110.    36.45  16.     2.  ]
State: [110.    36.45  16.     2.  ], Action: 2, Reward: -364.5, Next state: [120.    37.99  17.     2.  ]
State: [120.    37.99  17.     2.  ], Action: 1, Reward: -379.90000000000003, Next state: [130.  53.  18.   2.]
State: [130.  53.  18.   2.], Action: 2, Reward: -530.0, Next state: [140.    59.69  19.     2.  ]
State: [140.    59.69  19.     2.  ], Action: 2, Reward: -596.9, Next state: [150.    50.09  20.     2.  ]
State: [150.    50.09  20.     2.  ], Action: 0, Reward: -0.0, Next state: [150.  50.  21.   2.]
State: [150.  50.  21.   2.], Action: 0, Reward: -0.0, Next state: [150.    36.22  22.     2.  ]
State: [150.    36.22  22.     2.  ], Action: 2, Reward: -362.2, Next state: [160.    31.09  23.     2.  ]
State: [160.    31.09  23.     2.  ], Action: 2, Reward: -310.9, Next state: [170.    29.84  24.     2.  ]
State: [170.    29.84  24.     2.  ], Action: 2, Reward: -298.4, Next state: [50. 28.  1.  3.]
State: [50. 28.  1.  3.],